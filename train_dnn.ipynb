{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911ef91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a33fc18e",
   "metadata": {},
   "source": [
    "### argparse.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def args_parser():\n",
    "    parser = argparse.ArgumentParser(description='FMNIST baseline')\n",
    "    parser.add_argument('--name', '-n',\n",
    "                        default=\"default\",\n",
    "                        type=str,\n",
    "                        help='experiment name, used for saving results')\n",
    "    parser.add_argument('--backend',\n",
    "                        default=\"gloo\",\n",
    "                        type=str,\n",
    "                        help='backend name')\n",
    "    parser.add_argument('--out_fname',\n",
    "                        default=\".\",\n",
    "                        type=str,\n",
    "                        help='where to store log files')\n",
    "    parser.add_argument('--model',\n",
    "                        default=\"MLP\",\n",
    "                        type=str,\n",
    "                        help='neural network model')\n",
    "    parser.add_argument('--alpha',\n",
    "                        default=0.2,\n",
    "                        type=float,\n",
    "                        help='control the non-iidness of dataset')\n",
    "    parser.add_argument('--num_classes',\n",
    "                        type=int,\n",
    "                        default=10,\n",
    "                        help='number of classes')\n",
    "    parser.add_argument('--gmf',\n",
    "                        default=0,\n",
    "                        type=float,\n",
    "                        help='global (server) momentum factor')\n",
    "    parser.add_argument('--lr',\n",
    "                        default=0.1,\n",
    "                        type=float,\n",
    "                        help='client learning rate')\n",
    "    parser.add_argument('--momentum',\n",
    "                        default=0.0,\n",
    "                        type=float,\n",
    "                        help='local (client) momentum factor')\n",
    "    parser.add_argument('--bs',\n",
    "                        default=64,\n",
    "                        type=int,\n",
    "                        help='batch size on each worker/client')\n",
    "    parser.add_argument('--rounds',\n",
    "                        default=500,\n",
    "                        type=int,\n",
    "                        help='total communication rounds')\n",
    "    parser.add_argument('--localE',\n",
    "                        default=30,\n",
    "                        type=int,\n",
    "                        help='number of local epochs')\n",
    "    parser.add_argument('--decay',\n",
    "                        default=True,\n",
    "                        type=bool,\n",
    "                        help='1: decay LR, 0: no decay')\n",
    "    parser.add_argument('--print_freq',\n",
    "                        default=100,\n",
    "                        type=int,\n",
    "                        help='print info frequency')\n",
    "    parser.add_argument('--size',\n",
    "                        default=3,\n",
    "                        type=int,\n",
    "                        help='number of local workers')\n",
    "    parser.add_argument('--powd',\n",
    "                        default=6,\n",
    "                        type=int,\n",
    "                        help='number of selected subset workers per round ($d$)')\n",
    "    parser.add_argument('--fracC',\n",
    "                        default=0.03,\n",
    "                        type=float,\n",
    "                        help='fraction of selected workers per round')\n",
    "    parser.add_argument('--seltype',\n",
    "                        default='rand',\n",
    "                        type=str,\n",
    "                        help='type of client selection ($\\pi$)')\n",
    "    parser.add_argument('--ensize',\n",
    "                        default=100,\n",
    "                        type=int,\n",
    "                        help='number of all workers')\n",
    "    parser.add_argument('--rank',\n",
    "                        default=0,\n",
    "                        type=int,\n",
    "                        help='the rank of worker')\n",
    "    parser.add_argument('--rnd_ratio',\n",
    "                        default=0.1,\n",
    "                        type=float,\n",
    "                        help='hyperparameter for afl')\n",
    "    parser.add_argument('--delete_ratio',\n",
    "                        default=0.75,\n",
    "                        type=float,\n",
    "                        help='hyperparameter for afl')\n",
    "    parser.add_argument('--seed',\n",
    "                        default=1,\n",
    "                        type=int,\n",
    "                        help='random seed')\n",
    "    parser.add_argument('--save', '-s',\n",
    "                        action='store_true',\n",
    "                        help='whether save the training results')\n",
    "    parser.add_argument('--p', '-p',\n",
    "                        action='store_true',\n",
    "                        help='whether the dataset is partitioned or not')\n",
    "    parser.add_argument('--NIID',\n",
    "                        action='store_true',\n",
    "                        help='whether the dataset is non-iid or not')\n",
    "    parser.add_argument('--commE',\n",
    "                        action='store_true',\n",
    "                        help='activation of $cpow-d$')\n",
    "    parser.add_argument('--constantE',\n",
    "                        action='store_true',\n",
    "                        help='whether all the local workers have an identical \\\n",
    "                        number of local epochs or not')\n",
    "    parser.add_argument('--optimizer',\n",
    "                        default='local',\n",
    "                        type=str,\n",
    "                        help='optimizer name')\n",
    "    parser.add_argument('--initmethod',\n",
    "                        default='env://',\n",
    "                        type=str,\n",
    "                        help='init method')\n",
    "    parser.add_argument('--mu',\n",
    "                        default=0,\n",
    "                        type=float,\n",
    "                        help='mu parameter in fedprox')\n",
    "    parser.add_argument('--dataset',\n",
    "                        default='fmnist',\n",
    "                        type=str,\n",
    "                        help='type of dataset')\n",
    "    parser.add_argument('--img_size',\n",
    "                        default=32,\n",
    "                        type=int,\n",
    "                        help='image size')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7e2fcf",
   "metadata": {},
   "source": [
    "### Models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c303f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f14a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hidden1, dim_hidden2, dim_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer_input = nn.Linear(dim_in, dim_hidden1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.layer_hidden1 = nn.Linear(dim_hidden1, dim_hidden2)\n",
    "        self.layer_hidden2 = nn.Linear(dim_hidden2, dim_out)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1] * x.shape[-2] * x.shape[-1])\n",
    "        x = self.layer_input(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_hidden2(x)\n",
    "\n",
    "        return self.logsoftmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNMnist(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CNNMnist, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(args.num_channels, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, args.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, x.shape[1] * x.shape[2] * x.shape[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f597852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFashion_Mnist(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CNNFashion_Mnist, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(7 * 7 * 32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a0c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNCifar(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CNNCifar, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, args.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelC(nn.Module):\n",
    "    def __init__(self, input_size, n_classes=10):\n",
    "        super(modelC, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size, 96, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(96, 96, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(96, 96, 3, padding=1, stride=2)\n",
    "        self.conv4 = nn.Conv2d(96, 192, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(192, 192, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(192, 192, 3, padding=1, stride=2)\n",
    "        self.conv7 = nn.Conv2d(192, 192, 3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(192, 192, 1)\n",
    "\n",
    "        self.class_conv = nn.Conv2d(192, n_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_drop = F.dropout(x, .2)\n",
    "        conv1_out = F.relu(self.conv1(x_drop))\n",
    "        conv2_out = F.relu(self.conv2(conv1_out))\n",
    "        conv3_out = F.relu(self.conv3(conv2_out))\n",
    "        conv3_out_drop = F.dropout(conv3_out, .5)\n",
    "        conv4_out = F.relu(self.conv4(conv3_out_drop))\n",
    "        conv5_out = F.relu(self.conv5(conv4_out))\n",
    "        conv6_out = F.relu(self.conv6(conv5_out))\n",
    "        conv6_out_drop = F.dropout(conv6_out, .5)\n",
    "        conv7_out = F.relu(self.conv7(conv6_out_drop))\n",
    "        conv8_out = F.relu(self.conv8(conv7_out))\n",
    "\n",
    "        class_out = F.relu(self.class_conv(conv8_out))\n",
    "        pool_out = F.adaptive_avg_pool2d(class_out, 1)\n",
    "        pool_out.squeeze_(-1)\n",
    "        pool_out.squeeze_(-1)\n",
    "        return pool_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
    "           'vgg19_bn', 'vgg19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3f54de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f5e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 1\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8af6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',\n",
    "          512, 512, 512, 512, 'M'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg11():\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
    "    return VGG(make_layers(cfg['A']))\n",
    "\n",
    "def vgg11_bn():\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['A'], batch_norm=True))\n",
    "\n",
    "def vgg13():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\")\"\"\"\n",
    "    return VGG(make_layers(cfg['B']))\n",
    "\n",
    "def vgg13_bn():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['B'], batch_norm=True))\n",
    "\n",
    "def vgg16():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\")\"\"\"\n",
    "    return VGG(make_layers(cfg['D']))\n",
    "\n",
    "def vgg16_bn():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['D'], batch_norm=True))\n",
    "\n",
    "def vgg19():\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\"\"\"\n",
    "    return VGG(make_layers(cfg['E']))\n",
    "\n",
    "def vgg19_bn():\n",
    "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['E'], batch_norm=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328668b",
   "metadata": {},
   "source": [
    "### Utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc987985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data.distributed\n",
    "import torchvision\n",
    "from numpy.random import RandomState\n",
    "from torchvision import transforms\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ebfff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Partition(object):\n",
    "    \"\"\" Dataset-like object, but only access a subset of it. \"\"\"\n",
    "\n",
    "    def __init__(self, data, index):\n",
    "        self.data = data\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_idx = self.index[index]\n",
    "        return self.data[data_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPartitioner(object):\n",
    "    \"\"\" Partitions a dataset into different chunks. \"\"\"\n",
    "\n",
    "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], rnd=0, seed=1234, isNonIID=False, alpha=0,\n",
    "                 dataset=None, print_f=50):\n",
    "        self.data = data\n",
    "        self.dataset = dataset\n",
    "\n",
    "        if isNonIID:\n",
    "            self.partitions, self.ratio, self.dat_stat, self.endat_size = self.__getDirichletData__(data, sizes,\n",
    "                                                                                                    alpha, rnd, print_f)\n",
    "\n",
    "        else:\n",
    "            self.partitions = []\n",
    "            self.ratio = sizes\n",
    "            rng = Random()\n",
    "            rng.seed(seed)  # seed is fixed so same random number is generated\n",
    "            data_len = len(data)\n",
    "            indexes = [x for x in range(0, data_len)]\n",
    "            rng.shuffle(indexes)  # Same shuffling (with each seed)\n",
    "\n",
    "            for frac in sizes:\n",
    "                part_len = int(frac * data_len)\n",
    "                self.partitions.append(indexes[0:part_len])\n",
    "                indexes = indexes[part_len:]\n",
    "\n",
    "    def use(self, partition):\n",
    "        return Partition(self.data, self.partitions[partition])\n",
    "\n",
    "    def __getNonIIDdata__(self, data, sizes, seed, alpha):\n",
    "        labelList = data.train_labels\n",
    "        rng = Random()\n",
    "        rng.seed(seed)\n",
    "        a = [(label, idx) for idx, label in enumerate(labelList)]\n",
    "\n",
    "        # Same Part\n",
    "        labelIdxDict = dict()\n",
    "        for label, idx in a:\n",
    "            labelIdxDict.setdefault(label, [])\n",
    "            labelIdxDict[label].append(idx)\n",
    "        labelNum = len(labelIdxDict)\n",
    "        labelNameList = [key for key in labelIdxDict]\n",
    "        labelIdxPointer = [0] * labelNum\n",
    "\n",
    "        # sizes = number of nodes\n",
    "        partitions = [list() for i in range(len(sizes))]\n",
    "        eachPartitionLen = int(len(labelList) / len(sizes))\n",
    "\n",
    "        # majorLabelNumPerPartition = ceil(labelNum/len(partitions))\n",
    "        majorLabelNumPerPartition = 2\n",
    "        basicLabelRatio = alpha\n",
    "        interval = 1\n",
    "        labelPointer = 0\n",
    "\n",
    "        # basic part\n",
    "        for partPointer in range(len(partitions)):\n",
    "            requiredLabelList = list()\n",
    "            for _ in range(majorLabelNumPerPartition):\n",
    "                requiredLabelList.append(labelPointer)\n",
    "                labelPointer += interval\n",
    "                if labelPointer > labelNum - 1:\n",
    "                    labelPointer = interval\n",
    "                    interval += 1\n",
    "            for labelIdx in requiredLabelList:\n",
    "                start = labelIdxPointer[labelIdx]\n",
    "                idxIncrement = int(basicLabelRatio * len(labelIdxDict[labelNameList[labelIdx]]))\n",
    "                partitions[partPointer].extend(labelIdxDict[labelNameList[labelIdx]][start:start + idxIncrement])\n",
    "                labelIdxPointer[labelIdx] += idxIncrement\n",
    "\n",
    "        # random part\n",
    "        remainLabels = list()\n",
    "        for labelIdx in range(labelNum):\n",
    "            remainLabels.extend(labelIdxDict[labelNameList[labelIdx]][labelIdxPointer[labelIdx]:])\n",
    "        rng.shuffle(remainLabels)\n",
    "        for partPointer in range(len(partitions)):\n",
    "            idxIncrement = eachPartitionLen - len(partitions[partPointer])\n",
    "            partitions[partPointer].extend(remainLabels[:idxIncrement])\n",
    "            rng.shuffle(partitions[partPointer])\n",
    "            remainLabels = remainLabels[idxIncrement:]\n",
    "\n",
    "        return partitions\n",
    "\n",
    "    def __getDirichletData__(self, data, psizes, alpha, rnd, print_f):\n",
    "        n_nets = len(psizes)\n",
    "        K = 10\n",
    "        labelList = np.array(data.train_labels)\n",
    "        min_size = 0\n",
    "        N = len(labelList)\n",
    "        rann = RandomState(2020)\n",
    "\n",
    "        net_dataidx_map = {}\n",
    "        while min_size < K:\n",
    "            idx_batch = [[] for _ in range(n_nets)]\n",
    "            # for each class in the dataset\n",
    "            for k in range(K):\n",
    "                idx_k = np.where(labelList == k)[0]\n",
    "                rann.shuffle(idx_k)\n",
    "                proportions = rann.dirichlet(np.repeat(alpha, n_nets))\n",
    "                ## Balance\n",
    "                proportions = np.array([p * (len(idx_j) < N / n_nets) for p, idx_j in zip(proportions, idx_batch)])\n",
    "                proportions = proportions / proportions.sum()\n",
    "                proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]\n",
    "                idx_batch = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch, np.split(idx_k, proportions))]\n",
    "                min_size = min([len(idx_j) for idx_j in idx_batch])\n",
    "\n",
    "        for j in range(n_nets):\n",
    "            rann.shuffle(idx_batch[j])\n",
    "            net_dataidx_map[j] = idx_batch[j]\n",
    "\n",
    "        net_cls_counts = {}\n",
    "\n",
    "        for net_i, dataidx in net_dataidx_map.items():\n",
    "            unq, unq_cnt = np.unique(labelList[dataidx], return_counts=True)\n",
    "            tmp = {unq[i]: unq_cnt[i] for i in range(len(unq))}\n",
    "            net_cls_counts[net_i] = tmp\n",
    "\n",
    "        local_sizes = []\n",
    "        for i in range(n_nets):\n",
    "            local_sizes.append(len(net_dataidx_map[i]))\n",
    "        local_sizes = np.array(local_sizes)\n",
    "        weights = local_sizes / np.sum(local_sizes)\n",
    "\n",
    "        if rnd % print_f == 0:\n",
    "            print('Data statistics: %s' % str(net_cls_counts))\n",
    "            print('Data ratio: %s' % str(weights))\n",
    "\n",
    "        return idx_batch, weights, net_cls_counts, np.sum(local_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70746a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_dataset(size, args, rnd):\n",
    "    if args.dataset == 'cifar':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                                train=True,\n",
    "                                                download=True,\n",
    "                                                transform=transform_train)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                                   batch_size=64,\n",
    "                                                   shuffle=False,\n",
    "                                                   num_workers=size)\n",
    "\n",
    "        partition_sizes = [1.0 / args.ensize for _ in range(args.ensize)]\n",
    "        partition = DataPartitioner(trainset, partition_sizes, rnd, isNonIID=args.NIID, alpha=args.alpha,\n",
    "                                    dataset=args.dataset, print_f=args.print_freq)\n",
    "        ratio = partition.ratio\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                               train=False,\n",
    "                                               download=True,\n",
    "                                               transform=transform_test)\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(testset,\n",
    "                                                  batch_size=64,\n",
    "                                                  shuffle=False,\n",
    "                                                  num_workers=size)\n",
    "\n",
    "    elif args.dataset == 'fmnist':\n",
    "        apply_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root='./data',\n",
    "                                                     train=True,\n",
    "                                                     download=True,\n",
    "                                                     transform=apply_transform)\n",
    "\n",
    "        #train_sampler = \\\n",
    "        #    torch.utils.data.distributed.DistributedSampler(trainset,\n",
    "        #                                                    num_replicas=dist.get_world_size(),\n",
    "        #                                                    rank=dist.get_rank())\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                                   batch_size=64,\n",
    "                                                   shuffle=True, #train_sampler is None,\n",
    "                                                   #sampler=train_sampler,\n",
    "                                                   num_workers=size)\n",
    "\n",
    "        partition_sizes = [1.0 / args.ensize for _ in range(args.ensize)]\n",
    "        partition = DataPartitioner(trainset, partition_sizes, rnd, isNonIID=args.NIID, alpha=args.alpha,\n",
    "                                    dataset=args.dataset, print_f=args.print_freq)\n",
    "        ratio = partition.ratio  # Ratio of data sizes\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root='./data',\n",
    "                                                    train=False,\n",
    "                                                    download=True,\n",
    "                                                    transform=apply_transform)\n",
    "        test_loader = torch.utils.data.DataLoader(testset,\n",
    "                                                  batch_size=64,\n",
    "                                                  shuffle=False,\n",
    "                                                  num_workers=size)\n",
    "\n",
    "    elif args.dataset == 'emnist':\n",
    "        apply_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "        trainset = torchvision.datasets.EMNIST(root='./data',\n",
    "                                               split='digits',\n",
    "                                               train=True,\n",
    "                                               download=True,\n",
    "                                               transform=apply_transform)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                                   batch_size=64,\n",
    "                                                   shuffle=False,\n",
    "                                                   num_workers=size)\n",
    "\n",
    "        partition_sizes = [1.0 / args.ensize for _ in range(args.ensize)]\n",
    "        partition = DataPartitioner(trainset, partition_sizes, rnd, isNonIID=args.NIID, alpha=args.alpha,\n",
    "                                    dataset=args.dataset, print_f=args.print_freq)\n",
    "        ratio = partition.ratio  # Ratio of data sizes\n",
    "\n",
    "        testset = torchvision.datasets.EMNIST(root='./data',\n",
    "                                              split='digits',\n",
    "                                              train=False,\n",
    "                                              download=True,\n",
    "                                              transform=apply_transform)\n",
    "        test_loader = torch.utils.data.DataLoader(testset,\n",
    "                                                  batch_size=64,\n",
    "                                                  shuffle=False,\n",
    "                                                  num_workers=size)\n",
    "\n",
    "    # add more datasets here\n",
    "\n",
    "    args.img_size = trainset[0][0].shape\n",
    "\n",
    "    return partition, train_loader, test_loader, ratio, partition.dat_stat, partition.endat_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59071644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partitiondata_loader(partition, rank, batch_size):\n",
    "    \"\"\"\n",
    "    single mini-batch loader\n",
    "    \"\"\"\n",
    "    partition = partition.use(rank)\n",
    "\n",
    "    data_idx = random.sample(range(len(partition)), k=int(min(batch_size, len(partition))))\n",
    "    partitioned = torch.utils.data.Subset(partition, indices=data_idx)\n",
    "    trainbatch_loader = torch.utils.data.DataLoader(partitioned,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    pin_memory=True)\n",
    "    return trainbatch_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c24658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_client(DataRatios, cli_loss, cli_val, args, rnd):\n",
    "    \"\"\"\n",
    "    Client selection part returning the indices the set $\\mathcal{S}$ and $\\mathcal{A}$\n",
    "    :param DataRatios: $p_k$\n",
    "    :param cli_loss: actual local loss F_k(w)\n",
    "    :param cli_val: proxy of the local loss\n",
    "    :param args: variable arguments\n",
    "    :param rnd: communication round index\n",
    "    :return: idxs_users (indices of $\\mathcal{S}$), rnd_idx (indices of $\\mathcal{A}$)\n",
    "    \"\"\"\n",
    "    # If reproducibility is needed\n",
    "    # rng1 = Random()\n",
    "    # rng1.seed(seed)\n",
    "\n",
    "    rnd_idx = []\n",
    "    if args.seltype == 'rand':\n",
    "        # random selection in proportion to $p_k$ with replacement\n",
    "        idxs_users = np.random.choice(args.ensize, p=DataRatios, size=args.size, replace=True)\n",
    "\n",
    "    elif args.seltype == 'randint':\n",
    "        # 'rand' for intermittent client availability\n",
    "        delete = 0.2\n",
    "        if (rnd % 2) == 0:\n",
    "            del_idx = np.random.choice(int(args.ensize / 2), size=int(delete * args.ensize / 2), replace=False)\n",
    "            search_idx = np.delete(np.arange(0, args.ensize / 2), del_idx)\n",
    "        else:\n",
    "            del_idx = np.random.choice(np.arange(args.ensize / 2, args.ensize), size=int(delete * args.ensize / 2),\n",
    "                                       replace=False)\n",
    "            search_idx = np.delete(np.arange(args.ensize / 2, args.ensize), del_idx)\n",
    "\n",
    "        idxs_users = np.random.choice(search_idx, p=[DataRatios[int(i)] for i in search_idx] / sum([DataRatios[int(i)]\n",
    "                                                                                                    for i in\n",
    "                                                                                                    search_idx]),\n",
    "                                      size=args.size, replace=True)\n",
    "\n",
    "    elif args.seltype == 'pow-d':\n",
    "        # standard power-of-choice strategy\n",
    "        rnd_idx = np.random.choice(args.ensize, p=DataRatios, size=args.powd, replace=False)\n",
    "        repval = list(zip([cli_loss[i] for i in rnd_idx], rnd_idx))\n",
    "        repval.sort(key=lambda x: x[0], reverse=True)\n",
    "        rep = list(zip(*repval))\n",
    "        idxs_users = rep[1][:int(args.size)]\n",
    "\n",
    "    elif args.seltype == 'rpow-d':\n",
    "        # computation/communication efficient variant of 'pow-d'\n",
    "        rnd_idx1 = np.random.choice(args.ensize, p=DataRatios, size=args.powd, replace=False)\n",
    "        repval = list(zip([cli_val[i] for i in rnd_idx1], rnd_idx1))\n",
    "        repval.sort(key=lambda x: x[0], reverse=True)\n",
    "        rep = list(zip(*repval))\n",
    "        idxs_users = rep[1][:int(args.size)]\n",
    "\n",
    "    elif args.seltype == 'pow-dint':\n",
    "        # 'pow-d' for intermittent client availability\n",
    "        delete = 0.2\n",
    "        if (rnd % 2) == 0:\n",
    "            del_idx = np.random.choice(int(args.ensize / 2), size=int(delete * args.ensize / 2), replace=False)\n",
    "            search_idx = list(np.delete(np.arange(0, args.ensize / 2), del_idx))\n",
    "        else:\n",
    "            del_idx = np.random.choice(np.arange(args.ensize / 2, args.ensize), size=int(delete * args.ensize / 2),\n",
    "                                       replace=False)\n",
    "            search_idx = list(np.delete(np.arange(args.ensize / 2, args.ensize), del_idx))\n",
    "\n",
    "        rnd_idx = np.random.choice(search_idx, p=[DataRatios[int(i)] for i in search_idx] / sum([DataRatios[int(i)]\n",
    "                                                                                                 for i in search_idx]),\n",
    "                                   size=args.powd, replace=False)\n",
    "\n",
    "        repval = list(zip([cli_loss[int(i)] for i in rnd_idx], rnd_idx))\n",
    "        repval.sort(key=lambda x: x[0], reverse=True)\n",
    "        rep = list(zip(*repval))\n",
    "        idxs_users = rep[1][:int(args.size)]\n",
    "\n",
    "    elif args.seltype == 'rpow-dint':\n",
    "        # 'rpow-d' for intermittent client availability\n",
    "        delete = 0.2\n",
    "        if (rnd % 2) == 0:\n",
    "            del_idx = np.random.choice(int(args.ensize / 2), size=int(delete * args.ensize / 2), replace=False)\n",
    "            search_idx = list(np.delete(np.arange(0, args.ensize / 2), del_idx))\n",
    "        else:\n",
    "            del_idx = np.random.choice(np.arange(args.ensize / 2, args.ensize), size=int(delete * args.ensize / 2),\n",
    "                                       replace=False)\n",
    "            search_idx = list(np.delete(np.arange(args.ensize / 2, args.ensize), del_idx))\n",
    "\n",
    "        rnd_idx = np.random.choice(search_idx, p=[DataRatios[int(i)] for i in search_idx] / sum([DataRatios[int(i)]\n",
    "                                                                                                 for i in search_idx]),\n",
    "                                   size=args.powd, replace=False)\n",
    "\n",
    "        repval = list(zip([cli_val[int(i)] for i in rnd_idx], rnd_idx))\n",
    "        repval.sort(key=lambda x: x[0], reverse=True)\n",
    "        rep = list(zip(*repval))\n",
    "        idxs_users = rep[1][:int(args.size)]\n",
    "\n",
    "    elif args.seltype == 'afl':\n",
    "        # benchmark strategy\n",
    "        soft_temp = 0.01\n",
    "        sorted_loss_idx = np.argsort(cli_val)\n",
    "\n",
    "        for j in sorted_loss_idx[:int(args.delete_ratio * args.ensize)]:\n",
    "            cli_val[j] = -np.inf\n",
    "\n",
    "        loss_prob = np.exp(soft_temp * cli_val) / sum(np.exp(soft_temp * cli_val))\n",
    "        idx1 = np.random.choice(int(args.ensize), p=loss_prob, size=int(np.floor((1 - args.rnd_ratio) * args.size)),\n",
    "                                replace=False)\n",
    "\n",
    "        new_idx = np.delete(np.arange(0, args.ensize), idx1)\n",
    "        idx2 = np.random.choice(new_idx, size=int(args.size - np.floor((1 - args.rnd_ratio) * args.size)),\n",
    "                                replace=False)\n",
    "\n",
    "        idxs_users = list(idx1) + list(idx2)\n",
    "\n",
    "    return idxs_users, rnd_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21327f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choices(population, weights=None, cum_weights=None, k=1):\n",
    "    \"\"\"Return a k sized list of population elements chosen with replacement.\n",
    "    If the relative weights or cumulative weights are not specified,\n",
    "    the selections are made with equal probability.\n",
    "    \"\"\"\n",
    "\n",
    "    if cum_weights is None:\n",
    "        if weights is None:\n",
    "            total = len(population)\n",
    "            result = []\n",
    "            for i in range(k):\n",
    "                random.seed(i)\n",
    "                result.extend(population[int(random.random() * total)])\n",
    "            return result\n",
    "        cum_weights = []\n",
    "        c = 0\n",
    "        for x in weights:\n",
    "            c += x\n",
    "            cum_weights.append(c)\n",
    "    elif weights is not None:\n",
    "        raise TypeError('Cannot specify both weights and cumulative weights')\n",
    "    if len(cum_weights) != len(population):\n",
    "        raise ValueError('The number of weights does not match the population')\n",
    "    total = cum_weights[-1]\n",
    "    hi = len(cum_weights) - 1\n",
    "    from bisect import bisect\n",
    "    result = []\n",
    "    for i in range(k):\n",
    "        random.seed(i)\n",
    "        result.extend(population[bisect(cum_weights, random.random() * total, 0, hi)])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be07dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meter(object):\n",
    "    \"\"\" Computes and stores the average, variance, and current value \"\"\"\n",
    "\n",
    "    def __init__(self, init_dict=None, ptag='Time', stateful=False,\n",
    "                 csv_format=True):\n",
    "        \"\"\"\n",
    "        :param init_dict: Dictionary to initialize meter values\n",
    "        :param ptag: Print tag used in __str__() to identify meter\n",
    "        :param stateful: Whether to store value history and compute MAD\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "        self.ptag = ptag\n",
    "        self.value_history = None\n",
    "        self.stateful = stateful\n",
    "        if self.stateful:\n",
    "            self.value_history = []\n",
    "        self.csv_format = csv_format\n",
    "        if init_dict is not None:\n",
    "            for key in init_dict:\n",
    "                try:\n",
    "                    # TODO: add type checking to init_dict values\n",
    "                    self.__dict__[key] = init_dict[key]\n",
    "                except Exception:\n",
    "                    print('(Warning) Invalid key {} in init_dict'.format(key))\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.std = 0\n",
    "        self.sqsum = 0\n",
    "        self.mad = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.sqsum += (val ** 2) * n\n",
    "        if self.count > 1:\n",
    "            self.std = ((self.sqsum - (self.sum ** 2) / self.count)\n",
    "                        / (self.count - 1)\n",
    "                        ) ** 0.5\n",
    "        if self.stateful:\n",
    "            self.value_history.append(val)\n",
    "            mad = 0\n",
    "            for v in self.value_history:\n",
    "                mad += abs(v - self.avg)\n",
    "            self.mad = mad / len(self.value_history)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.csv_format:\n",
    "            if self.stateful:\n",
    "                return str('{dm.val:.3f},{dm.avg:.3f},{dm.mad:.3f}'\n",
    "                           .format(dm=self))\n",
    "            else:\n",
    "                return str('{dm.val:.3f},{dm.avg:.3f},{dm.std:.3f}'\n",
    "                           .format(dm=self))\n",
    "        else:\n",
    "            if self.stateful:\n",
    "                return str(self.ptag) + \\\n",
    "                    str(': {dm.val:.3f} ({dm.avg:.3f} +- {dm.mad:.3f})'\n",
    "                        .format(dm=self))\n",
    "            else:\n",
    "                return str(self.ptag) + \\\n",
    "                    str(': {dm.val:.3f} ({dm.avg:.3f} +- {dm.std:.3f})'\n",
    "                        .format(dm=self))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ce510",
   "metadata": {},
   "source": [
    "### comm_helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85469e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_tensors(tensors):\n",
    "    if len(tensors) == 1:\n",
    "        return tensors[0].view(-1).clone()\n",
    "    flat = torch.cat([t.view(-1) for t in tensors], dim=0)\n",
    "    return flat\n",
    "\n",
    "def unflatten_tensors(flat, tensors):\n",
    "    outputs = []\n",
    "    offset = 0\n",
    "    for tensor in tensors:\n",
    "        numel = tensor.numel()\n",
    "        outputs.append(flat.narrow(0, offset, numel).view_as(tensor))\n",
    "        offset += numel\n",
    "    return tuple(outputs)\n",
    "\n",
    "def communicate(tensors, communication_op, attention=False):\n",
    "    flat_tensor = flatten_tensors(tensors)\n",
    "    communication_op(tensor=flat_tensor)\n",
    "    if attention:\n",
    "        return tensors / flat_tensor\n",
    "    for f, t in zip(unflatten_tensors(flat_tensor, tensors), tensors):\n",
    "        t.set_(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911e59c",
   "metadata": {},
   "source": [
    "### FedAvg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from .comm_helpers import communicate, flatten_tensors, unflatten_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb29e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fedavg(Optimizer):\n",
    "    r\"\"\"Implements stochastic gradient descent for FedAvg.\"\"\"\n",
    "\n",
    "    def __init__(self, params, ratio, gmf, mu=0, lr=0.01, momentum=0, dampening=0,\n",
    "                 weight_decay=0, nesterov=False, variance=0):\n",
    "\n",
    "        self.gmf = gmf\n",
    "        self.ratio = ratio\n",
    "        self.etamu = mu * lr\n",
    "        self.mu = mu\n",
    "\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "\n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov, variance=variance)\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        super(fedavg, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(fedavg, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefargs.fracCault('nesterov', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "\n",
    "                param_state = self.state[p]\n",
    "                if 'old_init' not in param_state:\n",
    "                    param_state['old_init'] = torch.clone(p.data).detach()\n",
    "\n",
    "                local_lr = group['lr']\n",
    "\n",
    "                # apply momentum updates\n",
    "                if momentum != 0:\n",
    "                    if 'momentum_buffer' not in param_state:\n",
    "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                    else:\n",
    "                        buf = param_state['momentum_buffer']\n",
    "                        buf.mul_(momentum).add_(1 - dampening, d_p)\n",
    "                    if nesterov:\n",
    "                        d_p = d_p.add(momentum, buf)\n",
    "                    else:\n",
    "                        d_p = buf\n",
    "\n",
    "                # apply proximal updates\n",
    "                if self.etamu != 0:\n",
    "                    d_p.add_(self.mu, p.data - param_state['old_init'])\n",
    "\n",
    "                if 'cum_grad' not in param_state:\n",
    "                    param_state['cum_grad'] = torch.clone(d_p).detach()\n",
    "                    param_state['cum_grad'].mul_(local_lr)\n",
    "\n",
    "                else:\n",
    "                    param_state['cum_grad'].add_(local_lr, d_p)\n",
    "\n",
    "                p.data.add_(-local_lr, d_p)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def average(self, weight):\n",
    "        param_list = []\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                param_state = self.state[p]\n",
    "                param_state['cum_grad'].mul_(weight)\n",
    "                param_list.append(param_state['cum_grad'])\n",
    "\n",
    "        communicate(param_list, dist.all_reduce)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            for p in group['params']:\n",
    "                param_state = self.state[p]\n",
    "\n",
    "                if self.gmf != 0:\n",
    "                    if 'global_momentum_buffer' not in param_state:\n",
    "                        buf = param_state['global_momentum_buffer'] = torch.clone(param_state['cum_grad']).detach()\n",
    "                        buf.div_(lr)\n",
    "                    else:\n",
    "                        buf = param_state['global_momentum_buffer']\n",
    "                        buf.mul_(self.gmf).add_(1 / lr, param_state['cum_grad'])\n",
    "                    param_state['old_init'].sub_(lr, buf)\n",
    "                else:\n",
    "                    param_state['old_init'].sub_(param_state['cum_grad'])\n",
    "\n",
    "                p.data.copy_(param_state['old_init'])\n",
    "                param_state['cum_grad'].zero_()\n",
    "\n",
    "                # Reinitialize momentum buffer\n",
    "                if 'momentum_buffer' in param_state:\n",
    "                    param_state['momentum_buffer'].zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f0a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ddd7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a260b7d",
   "metadata": {},
   "source": [
    "### train_dnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265cd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf30018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76430516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pathlib\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c272fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.utils.data.distributed\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from distoptim import fedavg\n",
    "import util_v4 as util\n",
    "import models\n",
    "from params import args_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9911fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(levelname)s - %(message)s', level=logging.INFO)\n",
    "logging.debug('This message should appear on the console')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = args_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fbfb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(rank, size):\n",
    "    # initiate experiments folder\n",
    "    save_path = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/tsiameh1/code/Users/tsiameh/dnn'\n",
    "    fold = 'lr{:.4f}_bs{}_cp{}_a{:.2f}_e{}_r0_n{}_f{:.2f}/'.format(args.lr, args.bs, args.localE, args.alpha, args.seed,\n",
    "                                                                   args.ensize, args.fracC)\n",
    "    if args.commE:\n",
    "        fold = 'com_' + fold\n",
    "    folder_name = save_path + args.name + '/' + fold\n",
    "    file_name = '{}_rr{:.2f}_dr{:.2f}_lr{:.3f}_bs{:d}_cp{:d}_a{:.2f}_e{}_r{}_n{}_f{:.2f}_p{}.csv'.format(args.seltype,\n",
    "                                                                                                         args.rnd_ratio,\n",
    "                                                                                                         args.delete_ratio,\n",
    "                                                                                                         args.lr,\n",
    "                                                                                                         args.bs,\n",
    "                                                                                                         args.localE,\n",
    "                                                                                                         args.alpha,\n",
    "                                                                                                         args.seed,\n",
    "                                                                                                         rank,\n",
    "                                                                                                         args.ensize,\n",
    "                                                                                                         args.fracC,\n",
    "                                                                                                         args.powd)\n",
    "    pathlib.Path(folder_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # initiate log files\n",
    "    saveFileName = folder_name + file_name\n",
    "    args.out_fname = saveFileName\n",
    "    with open(args.out_fname, 'w+') as f:\n",
    "        print('BEGIN-TRAINING\\n' 'World-Size,{ws}\\n' 'Batch-Size,{bs}\\n' 'Epoch,itr,'\n",
    "              'loss,trainloss,avg:Loss,Prec@1,avg:Prec@1,val,trainval,updtime,comptime,seltime,entime'.format(\n",
    "            ws=args.size, bs=args.bs), file=f)\n",
    "\n",
    "    # seed for reproducibility\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # load data\n",
    "    partition, train_loader, test_loader, dataratios, datstat, endat = util.partition_dataset(size, args, 0)\n",
    "\n",
    "    # initialization for client selection\n",
    "    cli_loss, cli_freq, cli_val = np.zeros(args.ensize) + 1, np.zeros(args.ensize), np.zeros(args.ensize)\n",
    "\n",
    "    tmp_cli = [torch.tensor(0, dtype=torch.float32).cuda() for _ in range(dist.get_world_size())]\n",
    "    tmp_clifreq = [torch.tensor(0).cuda() for _ in range(dist.get_world_size())]\n",
    "\n",
    "    dist.barrier()\n",
    "    # select client for each round, in total m ranks\n",
    "    send = torch.zeros(args.size, dtype=torch.int32).cuda()\n",
    "    if rank == 0:\n",
    "        replace_param = False\n",
    "        if args.seltype == 'rand':\n",
    "            replace_param = True\n",
    "\n",
    "        idxs_users = np.random.choice(args.ensize, size=args.size, replace=replace_param)\n",
    "        send = [torch.tensor(int(ii)).cuda() for ii in idxs_users]\n",
    "    dist.barrier()\n",
    "\n",
    "    for i in range(args.size):\n",
    "        dist.broadcast(tensor=send[i], src=0)\n",
    "    dist.barrier()\n",
    "    sel_idx = int(send[rank])\n",
    "\n",
    "    # define neural nets model, criterion, and optimizer\n",
    "    if args.model == 'MLP':\n",
    "        len_in = 1\n",
    "        for x in args.img_size:\n",
    "            len_in *= x\n",
    "        model = models.MLP(dim_in=len_in, dim_hidden1=64, dim_hidden2=30, dim_out=args.num_classes).cuda()\n",
    "\n",
    "    else:\n",
    "        model = models.vgg11().cuda()  # vgg\n",
    "\n",
    "    criterion = nn.NLLLoss().cuda()\n",
    "\n",
    "    # select optimizer according to algorithm\n",
    "    algorithms = {'fedavg': fedavg}\n",
    "\n",
    "    selected_opt = algorithms[args.optimizer]\n",
    "    optimizer = selected_opt(model.parameters(),\n",
    "                             lr=args.lr,\n",
    "                             gmf=args.gmf,  # set to 0\n",
    "                             mu=args.mu,  # set to 0\n",
    "                             ratio=dataratios[rank],\n",
    "                             momentum=args.momentum,  # set to 0\n",
    "                             nesterov=False,\n",
    "                             weight_decay=1e-4)\n",
    "\n",
    "    for rnd in range(args.rounds):\n",
    "\n",
    "        # Initialize hyperparameters\n",
    "        local_epochs = args.localE\n",
    "        weight = 1 / args.size\n",
    "\n",
    "        # Decay learning rate according to round index (optional)\n",
    "        if args.decay:\n",
    "            update_learning_rate(optimizer, rnd, args.lr)\n",
    "\n",
    "        # Clients locally train for several local epochs\n",
    "        loss_final = 0\n",
    "        dist.barrier()\n",
    "        comm_update_start = time.time()\n",
    "        for t in range(local_epochs):\n",
    "            singlebatch_loader = util.partitiondata_loader(partition, sel_idx, args.bs)\n",
    "            loss = train(model, criterion, optimizer, singlebatch_loader, t)\n",
    "            loss_final += loss / local_epochs\n",
    "        dist.barrier()\n",
    "        comm_update_end = time.time()\n",
    "        update_time = comm_update_end - comm_update_start\n",
    "\n",
    "        # Getting value function for client selection (required only for 'rpow-d', 'afl')\n",
    "        dist.barrier()  # TODO: implement with multi-arm bandit\n",
    "        dist.all_gather(tmp_cli, torch.tensor(loss_final).cuda())\n",
    "        dist.all_gather(tmp_clifreq, torch.tensor(int(sel_idx)).cuda())\n",
    "        dist.barrier()\n",
    "        for i, i_val in enumerate(tmp_clifreq):\n",
    "            cli_freq[i_val.item()] += 1  # Cli freq is the entire clients that are selected for all rounds\n",
    "            cli_val[i_val.item()] = tmp_cli[i].item()\n",
    "        not_visited = np.where(cli_freq == 0)[0]\n",
    "\n",
    "        for ii in not_visited:\n",
    "            if args.seltype == 'afl':\n",
    "                cli_val[ii] = -np.inf\n",
    "            else:\n",
    "                cli_val[ii] = np.inf\n",
    "\n",
    "        # synchronize parameters\n",
    "        dist.barrier()\n",
    "        optimizer.average(weight=weight)\n",
    "        dist.barrier()\n",
    "\n",
    "        # evaluate test accuracy\n",
    "        test_acc, test_loss = evaluate(model, test_loader, criterion)\n",
    "\n",
    "        # evaluate loss values and sync selected frequency\n",
    "        cli_loss, cli_comptime = evaluate_client(model, criterion, partition)\n",
    "        train_loss = sum([cli_loss[i] * dataratios[i] for i in range(args.ensize)])\n",
    "        train_loss1 = sum(cli_loss) / args.ensize\n",
    "\n",
    "        dist.barrier()\n",
    "        # Select client for each round, in total m ranks\n",
    "        send = torch.zeros(args.size, dtype=torch.int32).cuda()\n",
    "        comp_time, sel_time = 0, 0\n",
    "\n",
    "        if rank == 0:\n",
    "            sel_time_start = time.time()\n",
    "            idxs_users, rnd_idx = util.sel_client(dataratios, cli_loss, cli_val, args, rnd)\n",
    "            sel_time_end = time.time()\n",
    "            sel_time = sel_time_end - sel_time_start\n",
    "\n",
    "            if args.seltype == 'pow-d' or args.seltype == 'pow-dint':\n",
    "                comp_time = max([cli_comptime[int(i)] for i in rnd_idx])\n",
    "\n",
    "            send = [torch.tensor(int(ii)).cuda() for ii in idxs_users]\n",
    "        dist.barrier()\n",
    "        for i in range(args.size):\n",
    "            dist.broadcast(tensor=send[i], src=0)\n",
    "        dist.barrier()\n",
    "        sel_idx = int(send[rank])\n",
    "\n",
    "        # record metrics\n",
    "        logging.info(\"Round {} rank {} test accuracy {:.3f} test loss {:.3f}\".format(rnd, rank, test_acc, test_loss))\n",
    "        with open(args.out_fname, '+a') as f:\n",
    "            print('{ep},{itr},{loss:.4f},{trainloss:.4f},{filler},'\n",
    "                  '{filler},{filler},'\n",
    "                  '{val:.4f},{other:.4f},{updtime:.4f},{comptime:.4f},{seltime:.4f},{entime:.4f}'\n",
    "                  .format(ep=rnd, itr=-1, loss=test_loss, trainloss=train_loss,\n",
    "                          filler=-1, val=test_acc, other=train_loss1, updtime=update_time, comptime=comp_time,\n",
    "                          seltime=sel_time, entime=update_time + comp_time + sel_time), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_client(model, criterion, partition):\n",
    "    \"\"\"\n",
    "    Evaluating each client's local loss values for the current global model for client selection\n",
    "    :param model: current global model\n",
    "    :param criterion: loss function\n",
    "    :param partition: dataset dict for clients\n",
    "    :return: cli_loss = list of local loss values, cli_comptime = list of computation time\n",
    "    \"\"\"\n",
    "\n",
    "    cli_comptime, cli_loss = [], []\n",
    "    model.eval()\n",
    "\n",
    "    # Get data from client to evaluate local loss on\n",
    "    for i in range(args.ensize):\n",
    "        partitioned = partition.use(i)\n",
    "\n",
    "        # cpow-d\n",
    "        if args.commE:\n",
    "            seldata_idx = random.sample(range(len(partitioned)), k=int(min(args.bs, len(partitioned))))\n",
    "            partitioned = torch.utils.data.Subset(partitioned, indices=seldata_idx)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(partitioned,\n",
    "                                                   batch_size=len(partitioned),\n",
    "                                                   shuffle=False,\n",
    "                                                   pin_memory=True)\n",
    "\n",
    "        # Compute local loss values or proxies for the clients\n",
    "        tmp, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            comptime_start = time.time()\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data = data.cuda(non_blocking=True)\n",
    "                target = target.cuda(non_blocking=True)\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, target)\n",
    "                tmp += loss.item()\n",
    "                total += 1\n",
    "            final_loss = tmp / total\n",
    "            comptime_end = time.time()\n",
    "            cli_comptime.append(comptime_end - comptime_start)\n",
    "            cli_loss.append(final_loss)\n",
    "\n",
    "    return cli_loss, cli_comptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c7fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate test accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "    # Get test accuracy for the current model\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data = data.cuda(non_blocking=True)\n",
    "            target = target.cuda(non_blocking=True)\n",
    "\n",
    "            # Inference\n",
    "            outputs = model(data)\n",
    "            batch_loss = criterion(outputs, target)\n",
    "            loss += batch_loss.item()\n",
    "\n",
    "            # Prediction\n",
    "            _, pred_labels = torch.max(outputs, 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            correct += torch.sum(torch.eq(pred_labels.view(-1), target)).item() / len(pred_labels)\n",
    "            total += 1\n",
    "\n",
    "        acc = (correct / total) * 100\n",
    "        los = loss / total\n",
    "\n",
    "    return acc, los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fba2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, loader, epoch):\n",
    "    \"\"\"\n",
    "    train model on the sampled mini-batch for $\\tau$ epochs\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        # data loading\n",
    "        data = data.cuda(non_blocking=True)\n",
    "        target = target.cuda(non_blocking=True)\n",
    "\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        batch_loss = criterion(output, target)\n",
    "\n",
    "        # backward pass\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # gradient clipping\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10, norm_type=2)\n",
    "\n",
    "        # gradient step\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # write log files\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        # Prediction\n",
    "        _, pred_labels = torch.max(output, 1)\n",
    "        correct += torch.sum(torch.eq(pred_labels.view(-1), target)).item() / len(pred_labels)\n",
    "        total += 1\n",
    "\n",
    "        acc = (correct / total) * 100\n",
    "        los = loss / total\n",
    "\n",
    "        if batch_idx % args.print_freq == 0 and args.save:\n",
    "            logging.debug('epoch {} itr {}, '\n",
    "                          'rank {}, loss value {:.4f}, train accuracy {:.3f}'\n",
    "                          .format(epoch, batch_idx, rank, los, acc))\n",
    "\n",
    "            with open(args.out_fname, '+a') as f:\n",
    "                print('{ep},{itr},'\n",
    "                      '{loss:.4f},-1,-1,'\n",
    "                      '{top1:.3f},-1,-1,-1,-1,-1,-1'\n",
    "                      .format(ep=epoch, itr=batch_idx,loss=los, top1=acc), file=f)\n",
    "\n",
    "    with open(args.out_fname, '+a') as f:\n",
    "        print('{ep},{itr},'\n",
    "              '{loss:.4f},-1,-1,'\n",
    "              '{top1:.3f},-1,-1,-1,-1,-1,-1'\n",
    "              .format(ep=epoch, itr=batch_idx,loss=los, top1=acc), file=f)\n",
    "\n",
    "    return los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_learning_rate(optimizer, epoch, target_lr):\n",
    "    \"\"\"\n",
    "    Decay learning rate\n",
    "    ** note: target_lr is the reference learning rate from which to scale down\n",
    "    \"\"\"\n",
    "    if epoch == 149:\n",
    "        lr = target_lr / 2\n",
    "        logging.info('Updating learning rate to {}'.format(lr))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    if epoch == 299:\n",
    "        lr = target_lr / 4\n",
    "        logging.info('Updating learning rate to {}'.format(lr))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b42002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_processes(rank, size, fn):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "\n",
    "    dist.init_process_group(backend=args.backend,\n",
    "                            timeout=datetime.timedelta(hours=5),\n",
    "                            init_method=args.initmethod,\n",
    "                            rank=rank,\n",
    "                            world_size=size)\n",
    "    fn(rank, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9082865",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    rank = args.rank\n",
    "    size = args.size\n",
    "\n",
    "    init_processes(rank, size, run)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
